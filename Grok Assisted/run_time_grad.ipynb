{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TimeGrad: Training and Inference on S&P500 Data\n",
    "\n",
    "This notebook demonstrates how to run the TimeGrad model on S&P500 time-series data. It covers:\n",
    "\n",
    "1.  **Data Fetching**: Downloading S&P500 data using `yfinance`.\n",
    "2.  **Data Preparation**: Normalizing and converting data into a GluonTS-compatible format.\n",
    "3.  **Model Training**: Training the TimeGrad model using a simplified Dataloader.\n",
    "4.  **Inference**: Generating synthetic time-series samples using the trained model.\n",
    "5.  **Visualization**: Comparing the synthetic data with the real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from typing import List\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Add the project's root directory to the Python path.\n",
    "# This allows us to import our custom modules like `data_fetch` and `src.pts`.\n",
    "module_path = os.path.abspath(os.getcwd()) # Assumes notebook is in 'Grok Assisted' directory\n",
    "if module_path not in sys.path:\n",
    "    sys.path.insert(0, module_path)\n",
    "\n",
    "from gluonts.dataset.split import split\n",
    "\n",
    "# Import from the modules directly\n",
    "from src.pts.model.time_grad.diffusion import Diffusion, DiffusionConfig\n",
    "from src.pts.model.time_grad.time_grad_network import TimeGradNetwork, NetworkConfig\n",
    "from src.pts.model.time_grad.time_grad_estimator import TimeGradEstimator, EstimatorConfig\n",
    "from src.pts.model.time_grad.time_grad_predictor import TimeGradPredictor\n",
    "from src.pts.trainer import Trainer\n",
    "from data_fetch import fetch_sp500_data, prepare_gluonts_dataset, DataConfig\n",
    "\n",
    "print(\"Imports successful and path is set up.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "We define all the configurations for data, network, diffusion, and the estimator in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data configuration\n",
    "data_config = DataConfig(start_date='2024-01-01', context_length=24, prediction_length=24)\n",
    "\n",
    "# Network configuration\n",
    "net_config = NetworkConfig(input_dim=1, hidden_dim=40, num_layers=2)\n",
    "\n",
    "# Diffusion process configuration\n",
    "diff_config = DiffusionConfig(num_steps=100, beta_start=1e-4, beta_end=0.1)\n",
    "\n",
    "# Estimator/Training configuration\n",
    "est_config = EstimatorConfig(learning_rate=1e-3, batch_size=64)\n",
    "\n",
    "print(\"Configurations loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Fetching and Preparation\n",
    "\n",
    "We fetch the S&P 500 data, normalize it, and prepare it as a GluonTS dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Fetching S&P500 data...\")\n",
    "    # Fetch data\n",
    "    df = fetch_sp500_data(data_config)\n",
    "    print(f\"Fetched {len(df)} data points\")\n",
    "\n",
    "    # Simple normalization\n",
    "    data_mean = df['close'].mean()\n",
    "    data_std = df['close'].std()\n",
    "    df_normalized = df.copy()\n",
    "    df_normalized['close'] = (df['close'] - data_mean) / data_std\n",
    "    print(f\"Data normalized - Mean: {data_mean:.2f}, Std: {data_std:.2f}\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = prepare_gluonts_dataset(df_normalized, data_config)\n",
    "    print(\"Dataset prepared successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during data fetching/preparation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Splitting and Dataloader\n",
    "\n",
    "The data is split into training and test sets. We then use a simple, custom data loader for training. This loader yields batches with the target data and initial hidden states for the RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train_dataset, test_dataset = split(dataset, offset=-data_config.prediction_length)\n",
    "train_data = list(train_dataset)\n",
    "\n",
    "print(f\"Training data: {len(train_data)} series\")\n",
    "\n",
    "# Simple dataloader for our MVP\n",
    "class SimpleLoader:\n",
    "    def __init__(self, ds, context_length, num_layers, hidden_dim):\n",
    "        self.ds = ds\n",
    "        self.context_length = context_length\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for item in self.ds:\n",
    "            target_data = item['target']\n",
    "            \n",
    "            # For simplicity, we take the last `context_length` as one sample\n",
    "            if len(target_data) >= self.context_length:\n",
    "                context_data = target_data[-self.context_length:]\n",
    "                # Reshape to (batch_size=1, seq_len, input_dim=1)\n",
    "                target_tensor = torch.tensor(\n",
    "                    context_data.reshape(1, -1, 1), \n",
    "                    dtype=torch.float32\n",
    "                )\n",
    "                \n",
    "                # Initial hidden state for the RNN\n",
    "                hidden = (\n",
    "                    torch.zeros(self.num_layers, 1, self.hidden_dim),\n",
    "                    torch.zeros(self.num_layers, 1, self.hidden_dim)\n",
    "                )\n",
    "                \n",
    "                yield {\n",
    "                    'target': target_tensor,\n",
    "                    'hidden': hidden\n",
    "                }\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "train_loader = SimpleLoader(\n",
    "    train_data, \n",
    "    data_config.context_length, \n",
    "    net_config.num_layers, \n",
    "    net_config.hidden_dim\n",
    ")\n",
    "\n",
    "print(\"Dataloader created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Initialization\n",
    "\n",
    "We initialize the core components of TimeGrad: the diffusion process, the noise prediction network, the estimator (which handles the loss), and the trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing models...\")\n",
    "# Initialize models\n",
    "diffusion = Diffusion(diff_config)\n",
    "network = TimeGradNetwork(net_config)\n",
    "estimator = TimeGradEstimator(est_config, network, diffusion)\n",
    "trainer = Trainer(estimator, epochs=10)\n",
    "\n",
    "print(\"Models initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training\n",
    "\n",
    "Now, we train the model. The progress bar will show the status for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training...\")\n",
    "losses = trainer.train(train_loader)\n",
    "print(f\"Training completed. Final loss: {losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inference: Generating Synthetic Samples\n",
    "\n",
    "With the trained network, we use the `TimeGradPredictor` to generate new time-series samples through the reverse diffusion process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating synthetic samples...\")\n",
    "# Generate samples\n",
    "predictor = TimeGradPredictor(network, diffusion)\n",
    "samples = predictor.predict(\n",
    "    context_length=data_config.prediction_length, \n",
    "    num_samples=100\n",
    ")\n",
    "print(f\"Generated samples shape: {samples.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization and Analysis\n",
    "\n",
    "Finally, we visualize the results. We denormalize the generated samples and plot them against the original data to qualitatively assess the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Helper class to mimic GluonTS/PyTorchTS Forecast object\n",
    "class SimpleForecast:\n",
    "    \"\"\"\n",
    "    A simplified forecast object that holds samples and can compute quantiles.\n",
    "    This is to make the generated samples compatible with plotting utilities\n",
    "    that expect a forecast-like object.\n",
    "    \"\"\"\n",
    "    def __init__(self, samples, start_date, freq):\n",
    "        # samples are expected to be denormalized, shape: (num_samples, prediction_length)\n",
    "        self.samples = samples\n",
    "        self.start_date = start_date\n",
    "        self.freq = freq\n",
    "        self.prediction_length = samples.shape[1]\n",
    "        \n",
    "        # Create a pandas DatetimeIndex for the forecast horizon\n",
    "        self.index = pd.date_range(\n",
    "            start=self.start_date, \n",
    "            periods=self.prediction_length, \n",
    "            freq=self.freq\n",
    "        )\n",
    "\n",
    "    def quantile(self, q: float) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute a quantile of the samples.\n",
    "        \n",
    "        Args:\n",
    "            q: The quantile to compute (e.g., 0.5 for median).\n",
    "            \n",
    "        Returns:\n",
    "            An array of shape (prediction_length,).\n",
    "        \"\"\"\n",
    "        return np.quantile(self.samples, q, axis=0)\n",
    "\n",
    "def plot_forecast(\n",
    "    target: pd.Series, \n",
    "    forecast: SimpleForecast, \n",
    "    prediction_length: int, \n",
    "    prediction_intervals: tuple = (50.0, 90.0), \n",
    "    color: str = 'g'\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots a univariate time series forecast against the target data.\n",
    "    \n",
    "    This is an adaptation of a more general plotting utility to fit the\n",
    "    univariate case of this notebook. It shows the median forecast and\n",
    "    prediction intervals.\n",
    "    \"\"\"\n",
    "    print(\"Creating forecast visualization...\")\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    ax = plt.gca()\n",
    "\n",
    "    # Plot the target data, showing some history before the forecast\n",
    "    # We plot the last 2 * prediction_length to give context\n",
    "    target_plot_range = target.iloc[-2 * prediction_length:]\n",
    "    target_plot_range.plot(ax=ax, label=\"Observations\", color=\"blue\")\n",
    "\n",
    "    # Define the percentiles to calculate for the prediction intervals\n",
    "    ps = [50.0] + [\n",
    "        50.0 + f * c / 2.0 for c in prediction_intervals for f in [-1.0, 1.0]\n",
    "    ]\n",
    "    percentiles_sorted = sorted(set(ps))\n",
    "\n",
    "    def alpha_for_percentile(p):\n",
    "        return (p / 100.0) ** 0.3\n",
    "\n",
    "    # Get quantile data from the forecast object\n",
    "    ps_data = [forecast.quantile(p / 100.0) for p in percentiles_sorted]\n",
    "    i_p50 = len(percentiles_sorted) // 2\n",
    "\n",
    "    # Plot the median forecast\n",
    "    p50_data = ps_data[i_p50]\n",
    "    p50_series = pd.Series(data=p50_data, index=forecast.index)\n",
    "    p50_series.plot(color=color, ls=\"-\", label=\"Median Forecast\", ax=ax)\n",
    "\n",
    "    # Plot the prediction intervals as shaded areas\n",
    "    for i in range(len(percentiles_sorted) // 2):\n",
    "        ptile = percentiles_sorted[i]\n",
    "        alpha = alpha_for_percentile(ptile)\n",
    "        ax.fill_between(\n",
    "            forecast.index,\n",
    "            ps_data[i],\n",
    "            ps_data[-i - 1],\n",
    "            facecolor=color,\n",
    "            alpha=alpha,\n",
    "            interpolate=True,\n",
    "        )\n",
    "        # Use a hack to create labels for the prediction intervals in the legend\n",
    "        pd.Series(data=p50_data[:1], index=forecast.index[:1]).plot(\n",
    "            color=color,\n",
    "            alpha=alpha,\n",
    "            linewidth=10,\n",
    "            label=f\"{100 - ptile * 2}% Interval\",\n",
    "            ax=ax,\n",
    "        )\n",
    "\n",
    "    ax.legend(loc=\"upper left\")\n",
    "    plt.title(\"S&P500 Forecast vs. Real Data\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.grid(which=\"both\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- Execution part ---\n",
    "\n",
    "# 1. Denormalize the generated samples\n",
    "denormalized_samples = samples.detach().numpy() * data_std + data_mean\n",
    "\n",
    "# 2. Determine the forecast start date and frequency\n",
    "# The forecast starts right after the training data ends.\n",
    "# We compare it against the last `prediction_length` part of our dataset.\n",
    "forecast_start_date = df.index[-data_config.prediction_length]\n",
    "freq = 'h' if data_config.interval == '1h' else 'D'\n",
    "\n",
    "# 3. Create the forecast object\n",
    "forecast_obj = SimpleForecast(\n",
    "    samples=denormalized_samples,\n",
    "    start_date=forecast_start_date,\n",
    "    freq=freq\n",
    ")\n",
    "\n",
    "# 4. Plot the forecast\n",
    "# The `target` is the full historical data series.\n",
    "plot_forecast(\n",
    "    target=df['close'], \n",
    "    forecast=forecast_obj, \n",
    "    prediction_length=data_config.prediction_length,\n",
    "    color='r' # Use red for the forecast\n",
    ")\n",
    "\n",
    "# 5. Print statistics for comparison\n",
    "original_data = df['close'][-data_config.prediction_length:].values\n",
    "median_forecast = forecast_obj.quantile(0.5)\n",
    "\n",
    "print(\"\\n=== Results Summary ===\")\n",
    "print(f\"Real data      - Mean: {np.mean(original_data):.2f}, Std: {np.std(original_data):.2f}\")\n",
    "print(f\"Median forecast - Mean: {np.mean(median_forecast):.2f}, Std: {np.std(median_forecast):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
